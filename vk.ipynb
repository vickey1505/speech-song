{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mWQBUFXG_DZD"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Install Libraries\n",
        "!pip install librosa soundfile numpy scikit-learn tensorflow matplotlib seaborn\n",
        "print(\"All required libraries installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted successfully!\")"
      ],
      "metadata": {
        "id": "w4FR9JU-_SZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Data Setup (Directly use unzipped folders)\n",
        "import os\n",
        "import shutil # Import shutil for robust directory management\n",
        "\n",
        "print(\"--- Step 3: Data Setup (Using Pre-Unzipped Data) ---\")\n",
        "\n",
        "# --- VERY IMPORTANT: DEFINE YOUR UNZIPPED FOLDER PATHS CORRECTLY ---\n",
        "# Example: If your unzipped 'Audio_Speech_Actors_01-24' and 'Audio_Song_Actors_01-24'\n",
        "# folders are directly inside 'My Drive/emotion_audio/'\n",
        "speech_data_dir = \"/content/drive/MyDrive/emotion_audio/Audio_Speech_Actors_01-24\"\n",
        "song_data_dir = \"/content/drive/MyDrive/emotion_audio/Audio_Song_Actors_01-24\"\n",
        "\n",
        "# Example: If your unzipped folders are in 'My Drive/My_Project_Data/Audio_Emotions/'\n",
        "# speech_data_dir = \"/content/drive/MyDrive/My_Project_Data/Audio_Emotions/Audio_Speech_Actors_01-24\"\n",
        "# song_data_dir = \"/content/drive/MyDrive/My_Project_Data/Audio_Emotions/Audio_Song_Actors_01-24\"\n",
        "\n",
        "print(f\"Set speech data directory to: {speech_data_dir}\")\n",
        "print(f\"Set song data directory to: {song_data_dir}\")\n",
        "\n",
        "# --- Verification of directory existence ---\n",
        "if not os.path.exists(speech_data_dir):\n",
        "    print(f\"ERROR: Speech data directory NOT FOUND at '{speech_data_dir}'. Please check the path carefully in Cell 3.\")\n",
        "    import sys\n",
        "    sys.exit(\"Exiting due to speech data directory not found.\")\n",
        "else:\n",
        "    print(f\"SUCCESS: Speech data directory found: {speech_data_dir}\")\n",
        "\n",
        "if not os.path.exists(song_data_dir):\n",
        "    print(f\"ERROR: Song data directory NOT FOUND at '{song_data_dir}'. Please check the path carefully in Cell 3.\")\n",
        "    import sys\n",
        "    sys.exit(\"Exiting due to song data directory not found.\")\n",
        "else:\n",
        "    print(f\"SUCCESS: Song data directory found: {song_data_dir}\")\n",
        "\n",
        "print(\"\\n--- Verification: Listing contents of data directories ---\")\n",
        "print(f\"Contents of {speech_data_dir}:\")\n",
        "list_speech_contents = os.listdir(speech_data_dir)\n",
        "if list_speech_contents:\n",
        "    print(f\"  Found {len(list_speech_contents)} items (e.g., {list_speech_contents[0]}, ...)\")\n",
        "    # Check if a typical Actor folder exists and list its contents\n",
        "    actor_dir_path = os.path.join(speech_data_dir, \"Actor_01\")\n",
        "    if os.path.exists(actor_dir_path):\n",
        "        print(f\"  Contents of {actor_dir_path}:\")\n",
        "        actor_contents = os.listdir(actor_dir_path)\n",
        "        if actor_contents:\n",
        "            # Print first 5 or all if less than 5\n",
        "            for i, item in enumerate(actor_contents[:5]):\n",
        "                print(f\"    - {item}\")\n",
        "            if len(actor_contents) > 5:\n",
        "                print(\"    ...\")\n",
        "        else:\n",
        "            print(f\"    '{actor_dir_path}' is empty.\")\n",
        "    else:\n",
        "        print(f\"  'Actor_01' directory not found directly in '{speech_data_dir}'. Structure might be different.\")\n",
        "else:\n",
        "    print(f\"  '{speech_data_dir}' is empty.\")\n",
        "\n",
        "print(f\"\\nContents of {song_data_dir}:\")\n",
        "list_song_contents = os.listdir(song_data_dir)\n",
        "if list_song_contents:\n",
        "    print(f\"  Found {len(list_song_contents)} items (e.g., {list_song_contents[0]}, ...)\")\n",
        "    actor_dir_path_song = os.path.join(song_data_dir, \"Actor_01\")\n",
        "    if os.path.exists(actor_dir_path_song):\n",
        "        print(f\"  Contents of {actor_dir_path_song}:\")\n",
        "        actor_contents_song = os.listdir(actor_dir_path_song)\n",
        "        if actor_contents_song:\n",
        "            for i, item in enumerate(actor_contents_song[:5]):\n",
        "                print(f\"    - {item}\")\n",
        "            if len(actor_contents_song) > 5:\n",
        "                print(\"    ...\")\n",
        "        else:\n",
        "            print(f\"    '{actor_dir_path_song}' is empty.\")\n",
        "    else:\n",
        "        print(f\"  'Actor_01' directory not found directly in '{song_data_dir}'. Structure might be different.\")\n",
        "else:\n",
        "    print(f\"  '{song_data_dir}' is empty.\")\n",
        "print(\"--- End Verification ---\")\n",
        "print(\"\\n--- Step 3 Complete: Audio data paths set and verified. ---\")"
      ],
      "metadata": {
        "id": "sOzOba6Y_SWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Data Loading and Metadata Extraction\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "print(\"\\n--- Step 4: Data Loading and Metadata Extraction ---\")\n",
        "\n",
        "# Function to extract emotion and other details from filename (RAVDESS dataset format)\n",
        "def extract_file_metadata(file_path):\n",
        "    # Example filename: 03-01-01-01-01-01-01.wav\n",
        "    basename = os.path.basename(file_path)\n",
        "    parts = basename.split('.')[0].split('-')\n",
        "\n",
        "    emotion_codes = {\n",
        "        '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',\n",
        "        '05': 'angry', '06': 'fearful', '07': 'disgust', '08': 'surprised'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        modality = int(parts[0])\n",
        "        vocal_channel = int(parts[1])\n",
        "        emotion_code = parts[2]\n",
        "        emotion = emotion_codes.get(emotion_code, 'unknown')\n",
        "        intensity = int(parts[3])\n",
        "        statement = int(parts[4])\n",
        "        repetition = int(parts[5])\n",
        "        actor = int(parts[6])\n",
        "\n",
        "        gender = 'male' if actor % 2 != 0 else 'female'\n",
        "\n",
        "        return {\n",
        "            'file_path': file_path,\n",
        "            'emotion': emotion,\n",
        "            'vocal_channel': 'speech' if vocal_channel == 1 else 'song',\n",
        "            'intensity': intensity,\n",
        "            'statement': statement,\n",
        "            'actor': actor,\n",
        "            'gender': gender\n",
        "        }\n",
        "    except (IndexError, ValueError) as e:\n",
        "        print(f\"WARNING: Could not parse filename '{basename}'. Skipping. Error: {e}\")\n",
        "        return None\n",
        "\n",
        "audio_data = []\n",
        "\n",
        "# Process speech files from the specified data directory\n",
        "for root, _, files in os.walk(speech_data_dir):\n",
        "    for file in files:\n",
        "        if file.endswith('.wav'):\n",
        "            file_path = os.path.join(root, file)\n",
        "            metadata = extract_file_metadata(file_path)\n",
        "            if metadata: # Only add if metadata extraction was successful\n",
        "                audio_data.append(metadata)\n",
        "\n",
        "# Process song files from the specified data directory\n",
        "for root, _, files in os.walk(song_data_dir):\n",
        "    for file in files:\n",
        "        if file.endswith('.wav'):\n",
        "            file_path = os.path.join(root, file)\n",
        "            metadata = extract_file_metadata(file_path)\n",
        "            if metadata:\n",
        "                audio_data.append(metadata)\n",
        "\n",
        "df = pd.DataFrame(audio_data)\n",
        "\n",
        "# --- VERIFICATION STEP FOR DATAFRAME CONTENTS ---\n",
        "print(f\"Total audio files found and added to DataFrame: {len(df)}\")\n",
        "if df.empty:\n",
        "    print(\"CRITICAL ERROR: DataFrame is empty. This means no WAV files were found or parsed correctly.\")\n",
        "    print(\"Please re-check Cell 3's output and verify the unzipped folder paths and contents.\")\n",
        "    import sys\n",
        "    sys.exit(\"Exiting because DataFrame is empty.\")\n",
        "else:\n",
        "    print(\"\\nDataFrame head (first 5 rows):\")\n",
        "    print(df.head())\n",
        "    print(\"\\nDistribution of emotions in the dataset:\")\n",
        "    print(df['emotion'].value_counts())\n",
        "    print(\"\\n--- Step 4 Complete: DataFrame populated. ---\")"
      ],
      "metadata": {
        "id": "u4gE4LoK_SQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Feature Extraction (MFCCs)\n",
        "import librosa\n",
        "import numpy as np\n",
        "# os is already imported from previous cells, but adding for clarity of this cell's dependencies\n",
        "\n",
        "print(\"\\n--- Step 5: Feature Extraction (MFCCs) ---\")\n",
        "\n",
        "# These will be set by the debugging snippet if successful\n",
        "mfcc_dim1 = None\n",
        "mfcc_dim2 = None\n",
        "\n",
        "def extract_mfccs(file_path, n_mfcc=40, target_duration=3, sr=22050):\n",
        "    \"\"\"\n",
        "    Extracts MFCCs from an audio file, resampling and padding/truncating\n",
        "    to ensure consistent output shape.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the audio file.\n",
        "        n_mfcc (int): Number of MFCCs to extract.\n",
        "        target_duration (int): Target duration in seconds for padding/truncation.\n",
        "        sr (int): Target sampling rate for resampling.\n",
        "\n",
        "    Returns:\n",
        "        np.array: Padded/truncated MFCCs, or None if an error occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load audio with original sampling rate first, then resample\n",
        "        y, original_sr = librosa.load(file_path, sr=None)\n",
        "\n",
        "        # Resample to the target sampling rate for consistency\n",
        "        if original_sr != sr:\n",
        "            y = librosa.resample(y=y, orig_sr=original_sr, target_sr=sr)\n",
        "\n",
        "        # Calculate target number of samples for padding/truncation\n",
        "        target_length_samples = int(sr * target_duration)\n",
        "\n",
        "        # Pad or truncate audio to the target length\n",
        "        if len(y) > target_length_samples:\n",
        "            y = y[:target_length_samples] # Truncate if longer\n",
        "        else:\n",
        "            y = np.pad(y, (0, max(0, target_length_samples - len(y))), \"constant\") # Pad if shorter\n",
        "\n",
        "        # Extract MFCCs\n",
        "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
        "\n",
        "        # Fixed target number of frames for MFCCs\n",
        "        # For RAVDESS, 3 seconds of audio at 22050Hz, with default hop_length=512\n",
        "        # typically results in around 130 frames (ceil(3 * 22050 / 512) = 130)\n",
        "        fixed_mfcc_frames = 130\n",
        "\n",
        "        if mfccs.shape[1] > fixed_mfcc_frames:\n",
        "            mfccs = mfccs[:, :fixed_mfcc_frames] # Truncate if more frames than target\n",
        "        else:\n",
        "            pad_width = fixed_mfcc_frames - mfccs.shape[1]\n",
        "            mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant') # Pad if fewer frames\n",
        "\n",
        "        return mfccs\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        # This error should ideally be caught earlier if paths in DataFrame are wrong\n",
        "        print(f\"ERROR: File not found during MFCC extraction: {file_path}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        # Catch any other librosa or numpy processing errors\n",
        "        print(f\"ERROR: Failed to process audio file '{file_path}': {e}\")\n",
        "        return None\n",
        "\n",
        "# --- IMPORTANT DEBUGGING SNIPPET FOR INITIAL MFCC EXTRACTION ---\n",
        "print(\"\\n--- Debugging MFCC Extraction for a single file ---\")\n",
        "if not df.empty:\n",
        "    first_file_path = df['file_path'].iloc[0]\n",
        "    print(f\"Attempting to process first file: {first_file_path}\")\n",
        "    try:\n",
        "        test_mfccs = extract_mfccs(first_file_path)\n",
        "        if test_mfccs is not None:\n",
        "            print(f\"SUCCESS: MFCCs extracted for first file. Shape: {test_mfccs.shape}\")\n",
        "            # Set global dimensions based on successful test\n",
        "            mfcc_dim1, mfcc_dim2 = test_mfccs.shape\n",
        "            print(f\"Determined MFCC dimensions: n_mfcc={mfcc_dim1}, time_frames={mfcc_dim2}\")\n",
        "        else:\n",
        "            print(f\"FATAL ERROR: Could not extract MFCCs for first file. Check error messages above.\")\n",
        "            print(\"This indicates a problem with the audio file or the 'extract_mfccs' function itself.\")\n",
        "            import sys\n",
        "            sys.exit(\"Exiting due to critical MFCC extraction failure on first file.\")\n",
        "    except Exception as e:\n",
        "        print(f\"FATAL ERROR: An unexpected error occurred during direct test of MFCC extraction for the first file: {e}\")\n",
        "        import sys\n",
        "        sys.exit(\"Exiting due to unexpected error during first file MFCC test.\")\n",
        "else:\n",
        "    print(\"FATAL ERROR: DataFrame is empty. No files to process for MFCC extraction. Please ensure Cell 4 ran successfully.\")\n",
        "    import sys\n",
        "    sys.exit(\"Exiting due to empty DataFrame for MFCC extraction.\")\n",
        "print(\"--- End Debugging Snippet ---\\n\")\n",
        "\n",
        "\n",
        "print(\"Starting full MFCC extraction for all files. This may take a few minutes...\")\n",
        "df['mfccs'] = df['file_path'].apply(extract_mfccs)\n",
        "\n",
        "# Remove rows where MFCC extraction failed (returns None)\n",
        "initial_audio_count = len(df)\n",
        "df.dropna(subset=['mfccs'], inplace=True)\n",
        "final_audio_count = len(df)\n",
        "\n",
        "print(f\"\\nTotal audio files initially in DataFrame: {initial_audio_count}\")\n",
        "print(f\"Total audio files successfully processed for MFCCs: {final_audio_count}\")\n",
        "if initial_audio_count > final_audio_count:\n",
        "    print(f\"WARNING: {initial_audio_count - final_audio_count} files failed MFCC extraction and were removed.\")\n",
        "\n",
        "# Final check for data availability after full processing\n",
        "if not df.empty:\n",
        "    # Ensure mfcc_dim1 and mfcc_dim2 are correctly set for subsequent cells\n",
        "    # They should already be set by the debug snippet, but ensure consistency\n",
        "    if not (mfcc_dim1, mfcc_dim2) == df['mfccs'].iloc[0].shape:\n",
        "        print(\"WARNING: MFCC dimensions changed after full processing. Resetting.\")\n",
        "        mfcc_dim1, mfcc_dim2 = df['mfccs'].iloc[0].shape\n",
        "    print(f\"MFCC dimensions for model input confirmed: n_mfcc={mfcc_dim1}, time_frames={mfcc_dim2}\")\n",
        "    print(\"\\n--- Step 5 Complete: MFCCs extracted. ---\")\n",
        "else:\n",
        "    print(\"CRITICAL ERROR: No valid MFCCs extracted after full processing. DataFrame is empty.\")\n",
        "    print(\"This implies a widespread issue with your audio files or the extraction function.\")\n",
        "    import sys\n",
        "    sys.exit(\"Exiting due to no valid MFCCs after full processing.\")"
      ],
      "metadata": {
        "id": "Pq2vMKzq_SEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Data Preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np # Ensure numpy is imported\n",
        "\n",
        "print(\"\\n--- Step 6: Data Preprocessing ---\")\n",
        "\n",
        "# Convert MFCCs list of arrays into a single NumPy array\n",
        "# X needs to be (samples, n_mfcc, time_frames) for Conv1D, or (samples, time_frames, n_mfcc)\n",
        "# Our current MFCCs are (n_mfcc, time_frames). So, X will be (samples, n_mfcc, time_frames)\n",
        "X = np.array(df['mfccs'].tolist())\n",
        "\n",
        "# Conv1D expects input shape (batch, timesteps, features) or (batch, features, timesteps)\n",
        "# If we treat n_mfcc as features and time_frames as timesteps: X.shape becomes (samples, time_frames, n_mfcc)\n",
        "# So, we should transpose the MFCCs to (time_frames, n_mfcc) before creating the array, or transpose X later.\n",
        "# Let's reshape X to (samples, time_frames, n_mfcc) as this is more common for Conv1D.\n",
        "X = X.transpose(0, 2, 1) # Transpose from (samples, n_mfcc, time_frames) to (samples, time_frames, n_mfcc)\n",
        "\n",
        "# Now add the channel dimension for Conv1D, usually 1 for mono audio features\n",
        "X = np.expand_dims(X, -1) # Shape: (samples, time_frames, n_mfcc, 1)\n",
        "\n",
        "# Ensure mfcc_dim1 and mfcc_dim2 are updated to reflect the time_frames and n_mfcc after transpose\n",
        "# For Conv1D input_shape will be (time_frames, n_mfcc, 1)\n",
        "mfcc_timesteps = X.shape[1] # This is now the time_frames\n",
        "mfcc_features = X.shape[2]  # This is now the n_mfcc\n",
        "input_shape_for_conv1d = (mfcc_timesteps, mfcc_features)\n",
        "\n",
        "print(f\"Adjusted MFCC dimensions for Conv1D input_shape: (timesteps, features) = {input_shape_for_conv1d}\")\n",
        "\n",
        "# Initialize LabelEncoder to convert emotion names (strings) to numerical labels\n",
        "le = LabelEncoder()\n",
        "# Fit LabelEncoder on all unique emotion names and transform them\n",
        "y_encoded = le.fit_transform(df['emotion'])\n",
        "# Convert numerical labels to one-hot encoded format\n",
        "y = to_categorical(y_encoded)\n",
        "\n",
        "num_classes = y.shape[1]\n",
        "print(f\"Number of emotion classes: {num_classes}\")\n",
        "print(f\"Emotion labels (original order): {le.classes_}\")\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y_encoded)\n",
        "\n",
        "print(f\"\\nShape of training features (X_train): {X_train.shape}\")\n",
        "print(f\"Shape of validation features (X_val): {X_val.shape}\")\n",
        "print(f\"Shape of training labels (y_train): {y_train.shape}\")\n",
        "print(f\"Shape of validation labels (y_val): {y_val.shape}\")\n",
        "print(\"\\n--- Step 6 Complete: Data preprocessed. ---\")"
      ],
      "metadata": {
        "id": "P1i-Vgvw_Ruu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Model Definition (CNN)\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam # Explicitly import Adam\n",
        "\n",
        "print(\"\\n--- Step 7: Model Definition (CNN) ---\")\n",
        "\n",
        "# Ensure mfcc_timesteps and mfcc_features are set from previous step (Cell 6)\n",
        "# These variables come from the shape of X_train after preprocessing in Cell 6.\n",
        "# If you run this cell independently, ensure Cell 6 has been run first.\n",
        "if 'mfcc_timesteps' not in globals() or 'mfcc_features' not in globals():\n",
        "    print(\"FATAL ERROR: MFCC dimensions (mfcc_timesteps, mfcc_features) not found. Please run Cell 6 first.\")\n",
        "    import sys\n",
        "    sys.exit(\"Exiting.\")\n",
        "\n",
        "model = Sequential([\n",
        "    # Input shape for Conv1D is (timesteps, features)\n",
        "    # X_train is (samples, time_frames, n_mfcc, 1) -> input_shape for Conv1D will be (time_frames, n_mfcc)\n",
        "    Conv1D(filters=128, kernel_size=7, activation='relu', input_shape=(mfcc_timesteps, mfcc_features)),\n",
        "    BatchNormalization(), # Added Batch Normalization after Conv1D\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3), # Adjusted dropout\n",
        "\n",
        "    Conv1D(filters=256, kernel_size=5, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Conv1D(filters=512, kernel_size=3, activation='relu'), # Smaller kernel for deeper layer\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'), # Increased Dense layer size\n",
        "    BatchNormalization(), # Added Batch Normalization for Dense layer\n",
        "    Dropout(0.6), # Increased dropout for dense layer to combat potential overfitting\n",
        "    Dense(num_classes, activation='softmax') # Output layer\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.0005), # Slightly lower initial learning rate\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Callbacks for better training\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True) # Increased patience\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=12, min_lr=0.000005) # Increased patience\n",
        "\n",
        "print(\"Model Architecture Summary:\")\n",
        "model.summary()\n",
        "print(\"\\n--- Step 7 Complete: Model defined. ---\")"
      ],
      "metadata": {
        "id": "ov8t9GL-_2Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Model Training\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import class_weight\n",
        "import numpy as np # Ensure numpy is imported\n",
        "\n",
        "print(\"\\n--- Step 8: Model Training ---\")\n",
        "\n",
        "# Compute class weights\n",
        "# y_encoded_train contains the integer labels for the training set\n",
        "# You need the integer labels, not one-hot encoded, for compute_class_weight\n",
        "# If y_train is one-hot, convert it back to integer labels first\n",
        "y_encoded_train_labels = np.argmax(y_train, axis=1)\n",
        "\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced', # 'balanced' automatically adjusts weights inversely proportional to class frequencies\n",
        "    classes=np.unique(y_encoded_train_labels),\n",
        "    y=y_encoded_train_labels\n",
        ")\n",
        "class_weights_dict = dict(enumerate(class_weights)) # Map integer label to its weight\n",
        "\n",
        "print(f\"Computed class weights: {class_weights_dict}\")\n",
        "# You might manually inspect these. If neutral is heavily underrepresented, its weight will be higher.\n",
        "# Even if balanced, 'balanced' weights can still help hard-to-classify classes.\n",
        "\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    epochs=100,\n",
        "                    batch_size=32,\n",
        "                    callbacks=[early_stopping, reduce_lr],\n",
        "                    class_weight=class_weights_dict, # ADD THIS LINE\n",
        "                    verbose=1)\n",
        "\n",
        "print(\"\\n--- Step 8 Complete: Training finished. ---\")\n",
        "\n",
        "# Plot training history (rest of the code remains the same)\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vg10SOTM_2E4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Model Evaluation\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score\n",
        "import matplotlib.pyplot as plt # Ensure plt is imported\n",
        "\n",
        "print(\"\\n--- Step 9: Model Evaluation ---\")\n",
        "\n",
        "# Evaluate on validation data\n",
        "val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Validation Overall Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "# Predictions for confusion matrix and F1 score\n",
        "y_pred_probs = model.predict(X_val)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "y_true = np.argmax(y_val, axis=1)\n",
        "\n",
        "# Convert numerical labels back to emotion names for clarity\n",
        "y_pred_labels = le.inverse_transform(y_pred)\n",
        "y_true_labels = le.inverse_transform(y_true)\n",
        "\n",
        "# --- Confusion Matrix ---\n",
        "print(\"\\n--- Confusion Matrix ---\")\n",
        "cm = confusion_matrix(y_true_labels, y_pred_labels, labels=le.classes_)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# --- Classification Report ---\n",
        "print(\"\\n--- Classification Report ---\")\n",
        "report = classification_report(y_true_labels, y_pred_labels, target_names=le.classes_, output_dict=True)\n",
        "print(classification_report(y_true_labels, y_pred_labels, target_names=le.classes_))\n",
        "\n",
        "# --- Calculate F1 Score (weighted average for multi-class) ---\n",
        "f1_weighted = f1_score(y_true_labels, y_pred_labels, average='weighted')\n",
        "print(f\"\\nWeighted F1 Score: {f1_weighted:.4f}\")\n",
        "\n",
        "# --- Check individual class accuracies ---\n",
        "print(\"\\n--- Individual Class Accuracies ---\")\n",
        "class_accuracies = {}\n",
        "for class_name in le.classes_:\n",
        "    true_for_class = y_true_labels == class_name\n",
        "    predicted_correctly_for_class = (y_pred_labels == class_name) & true_for_class\n",
        "    total_samples_in_class = np.sum(true_for_class)\n",
        "\n",
        "    if total_samples_in_class > 0:\n",
        "        class_accuracy = np.sum(predicted_correctly_for_class) / total_samples_in_class\n",
        "    else:\n",
        "        class_accuracy = 0 # Handle cases where a class might have no samples\n",
        "    class_accuracies[class_name] = class_accuracy\n",
        "    print(f\"Accuracy for class '{class_name}': {class_accuracy:.4f}\")\n",
        "\n",
        "print(\"\\n--- Project Evaluation Criteria Check ---\")\n",
        "print(f\"Overall Validation Accuracy: {val_accuracy * 100:.2f}% (Target: > 80%)\")\n",
        "print(f\"Weighted F1 Score: {f1_weighted * 100:.2f}% (Target: > 80%)\")\n",
        "\n",
        "all_class_accuracy_met = True\n",
        "for class_name, acc in class_accuracies.items():\n",
        "    if acc < 0.75:\n",
        "        all_class_accuracy_met = False\n",
        "        print(f\"FAIL: Class '{class_name}' accuracy ({acc * 100:.2f}%) is below 75%.\")\n",
        "    else:\n",
        "        print(f\"PASS: Class '{class_name}' accuracy ({acc * 100:.2f}%) is above 75%.\")\n",
        "\n",
        "if val_accuracy > 0.80 and f1_weighted > 0.80 and all_class_accuracy_met:\n",
        "    print(\"\\nCONGRATULATIONS! All project criteria on validation set are potentially met.\")\n",
        "else:\n",
        "    print(\"\\nKEEP WORKING! Not all project criteria on validation set are met yet.\")\n",
        "    print(\"Consider further tuning, data augmentation, or a more complex model.\")\n",
        "\n",
        "print(\"\\n--- Step 9 Complete: Evaluation finished. ---\")"
      ],
      "metadata": {
        "id": "j2aXFsqo_10R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Plot Accuracy (Final Plot)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot accuracy\n",
        "plt.plot(history.history['accuracy'], label='Train')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation')\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AQLr9NMBN_uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('vk.h5')"
      ],
      "metadata": {
        "id": "ZsiMqf2Jb166"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Save the Trained Model and LabelEncoder\n",
        "\n",
        "import os\n",
        "import joblib # A robust library for saving and loading Python objects\n",
        "\n",
        "print(\"\\n--- Step 11: Saving Model and LabelEncoder ---\")\n",
        "\n",
        "# Define paths to save your model and label encoder in your Google Drive\n",
        "# It's good practice to create a specific folder for deployment assets\n",
        "deployment_assets_dir = \"/content/drive/MyDrive/emotion_audio_deployment\"\n",
        "os.makedirs(deployment_assets_dir, exist_ok=True)\n",
        "\n",
        "model_save_path = os.path.join(deployment_assets_dir, \"ravdess_emotion_model.keras\")\n",
        "label_encoder_save_path = os.path.join(deployment_assets_dir, \"label_encoder.pkl\")\n",
        "\n",
        "try:\n",
        "    # 1. Save the Keras model\n",
        "    model.save(model_save_path)\n",
        "    print(f\"Model successfully saved to: {model_save_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Could not save the model: {e}\")\n",
        "\n",
        "try:\n",
        "    # 2. Save the LabelEncoder\n",
        "    # The 'le' object was created in Cell 6\n",
        "    joblib.dump(le, label_encoder_save_path)\n",
        "    print(f\"LabelEncoder successfully saved to: {label_encoder_save_path}\")\n",
        "except NameError:\n",
        "    print(\"ERROR: 'le' (LabelEncoder) variable not found. Please ensure Cell 6 has been run.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Could not save the LabelEncoder: {e}\")\n",
        "\n",
        "print(\"\\n--- Step 11 Complete: Assets saved for deployment. ---\")"
      ],
      "metadata": {
        "id": "gJqYv7zncmPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "# ... rest of your app.py code ..."
      ],
      "metadata": {
        "id": "qXXjYr2zeACQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st# app.py\n",
        "\n",
        "import streamlit as st\n",
        "import librosa\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "import joblib\n",
        "import os\n",
        "import io # To handle file-like objects from Streamlit uploader\n",
        "\n",
        "# --- 1. Load the Model and LabelEncoder ---\n",
        "# These paths are relative to where your app.py will be run or deployed.\n",
        "# Ensure your 'ravdess_emotion_model.keras' and 'label_encoder.pkl' are\n",
        "# in the same directory as your app.py file when deploying.\n",
        "MODEL_PATH = 'ravdess_emotion_model.keras'\n",
        "LABEL_ENCODER_PATH = 'label_encoder.pkl'\n",
        "\n",
        "# Check if model and encoder files exist (important for local testing and deployment)\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    st.error(f\"Error: Model file '{MODEL_PATH}' not found. Please ensure it's in the same directory.\")\n",
        "    st.stop() # Stop the app if crucial files are missing\n",
        "if not os.path.exists(LABEL_ENCODER_PATH):\n",
        "    st.error(f\"Error: LabelEncoder file '{LABEL_ENCODER_PATH}' not found. Please ensure it's in the same directory.\")\n",
        "    st.stop()\n",
        "\n",
        "@st.cache_resource # Cache the model loading for performance\n",
        "def load_my_model(path):\n",
        "    return load_model(path)\n",
        "\n",
        "@st.cache_resource # Cache the label encoder loading\n",
        "def load_my_label_encoder(path):\n",
        "    return joblib.load(path)\n",
        "\n",
        "model = load_my_model(MODEL_PATH)\n",
        "le = load_my_label_encoder(LABEL_ENCODER_PATH)\n",
        "\n",
        "# --- 2. Define Preprocessing Function (MUST MATCH Colab Cell 5 & 6) ---\n",
        "def preprocess_audio(audio_file_path_or_buffer, n_mfcc=40, target_duration=3, sr=22050):\n",
        "    \"\"\"\n",
        "    Extracts MFCCs from an audio file (or buffer), resampling and padding/truncating\n",
        "    to ensure consistent output shape, ready for model prediction.\n",
        "    This function MUST EXACTLY MATCH the logic in your Colab notebook's extract_mfccs\n",
        "    and subsequent reshaping.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # librosa.load can handle file paths or file-like objects (like BytesIO from st.file_uploader)\n",
        "        y, original_sr = librosa.load(audio_file_path_or_buffer, sr=None)\n",
        "\n",
        "        # Resample if necessary\n",
        "        if original_sr != sr:\n",
        "            y = librosa.resample(y=y, orig_sr=original_sr, target_sr=sr)\n",
        "\n",
        "        # Calculate target number of samples for padding/truncation\n",
        "        target_length_samples = int(sr * target_duration)\n",
        "\n",
        "        # Pad or truncate audio to the target length\n",
        "        if len(y) > target_length_samples:\n",
        "            y = y[:target_length_samples]\n",
        "        else:\n",
        "            y = np.pad(y, (0, max(0, target_length_samples - len(y))), \"constant\")\n",
        "\n",
        "        # Extract MFCCs\n",
        "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
        "\n",
        "        # Fixed target number of frames for MFCCs (from Colab Cell 5)\n",
        "        fixed_mfcc_frames = 130\n",
        "\n",
        "        if mfccs.shape[1] > fixed_mfcc_frames:\n",
        "            mfccs = mfccs[:, :fixed_mfcc_frames]\n",
        "        else:\n",
        "            pad_width = fixed_mfcc_frames - mfccs.shape[1]\n",
        "            mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "\n",
        "        # Reshape for Conv1D input (from Colab Cell 6)\n",
        "        # From (n_mfcc, time_frames) to (1, time_frames, n_mfcc, 1)\n",
        "        mfccs = mfccs.transpose(1, 0) # (time_frames, n_mfcc)\n",
        "        mfccs = np.expand_dims(mfccs, axis=0) # (1, time_frames, n_mfcc) - for batch\n",
        "        mfccs = np.expand_dims(mfccs, axis=-1) # (1, time_frames, n_mfcc, 1) - add channel dim\n",
        "\n",
        "        return mfccs\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error processing audio: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- 3. Streamlit UI ---\n",
        "st.title(\"Emotion Recognition from Speech/Song\")\n",
        "st.markdown(\"Upload an audio file (WAV) and let the CNN model predict its emotion!\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Choose an audio file...\", type=[\"wav\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    # Display uploaded audio\n",
        "    st.audio(uploaded_file, format='audio/wav')\n",
        "\n",
        "    # Create a BytesIO object from the uploaded file\n",
        "    audio_bytes = io.BytesIO(uploaded_file.read())\n",
        "\n",
        "    # Preprocess the audio\n",
        "    features = preprocess_audio(audio_bytes)\n",
        "\n",
        "    if features is not None:\n",
        "        st.write(\"Audio processing complete. Predicting emotion...\")\n",
        "        # Make prediction\n",
        "        prediction = model.predict(features)\n",
        "        predicted_class_index = np.argmax(prediction, axis=1)[0]\n",
        "        predicted_emotion = le.inverse_transform([predicted_class_index])[0]\n",
        "        confidence = np.max(prediction) * 100\n",
        "\n",
        "        st.subheader(\"Prediction Result:\")\n",
        "        st.success(f\"**Predicted Emotion:** {predicted_emotion.upper()}\")\n",
        "        st.info(f\"**Confidence:** {confidence:.2f}%\")\n",
        "\n",
        "        # Optional: Display all probabilities\n",
        "        st.markdown(\"---\")\n",
        "        st.subheader(\"All Emotion Probabilities:\")\n",
        "        prob_df = pd.DataFrame({\n",
        "            'Emotion': le.classes_,\n",
        "            'Probability': prediction[0] * 100\n",
        "        }).sort_values(by='Probability', ascending=False)\n",
        "        st.dataframe(prob_df.style.format({\"Probability\": \"{:.2f}%\"}), hide_index=True)\n",
        "\n",
        "\n",
        "    else:\n",
        "        st.error(\"Could not extract features from the audio file. Please try another file.\")\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\"Note: This model was trained on the RAVDESS dataset and performs best on similar audio.\")"
      ],
      "metadata": {
        "id": "2vii40PDcmSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L2gRT2AFcmXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oMrFKf2qcmag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "COpb5BBvcmdY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}